{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpFGT9UPTH2E"
      },
      "source": [
        "# Packages\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXVWzJ07ZxNj"
      },
      "source": [
        "!pip install seaborn\n",
        "!pip install matplotlib\n",
        "!pip install -U scikit-learn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a118o9l0ayZG"
      },
      "source": [
        "# Surrogate Model Building\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nq5V59DfajSi"
      },
      "source": [
        "def sigmoid(x):\n",
        "    return (1 / (1 + math.exp(-x)))\n",
        "def epsilons ():\n",
        "    return np.random.normal(loc = 0,scale = 0.5)\n",
        "def para_c1():\n",
        "    return np.random.normal(loc=-2,scale=1)\n",
        "def para_c2():\n",
        "    return np.random.normal(loc=2,scale=1)\n",
        "def para_cw_2():\n",
        "    return np.random.normal(loc=1,scale=1)\n",
        "\n",
        "def build_w1(u1,u2):\n",
        "    return sigmoid(para_c1()+u1+para_c2()+u2+epsilons())\n",
        "\n",
        "def build_w2(w1,d):\n",
        "    temp = 1.0\n",
        "    for i in range(d):\n",
        "        p = para_cw_2()\n",
        "        temp += (2*w1[i] - 1)*p\n",
        "    return sigmoid(-temp + epsilons())\n",
        "\n",
        "def build_x(u1,w2):\n",
        "    return sigmoid(u1 - 4*w2 + 2 + epsilons())\n",
        "\n",
        "def build_y(u2,x):\n",
        "    return sigmoid(0.5*u2-2*x+1+epsilons())\n",
        "\n",
        "def build_causal_model(n_samples,d):\n",
        "    scm_numpy = np.empty((n_samples,d+3))\n",
        "    col_name = list()\n",
        "    for i in range(n_samples):\n",
        "        u1 = np.random.normal(loc=0,scale=1)\n",
        "        u2 = np.random.normal(loc=0,scale=1)\n",
        "        w1_s = list()\n",
        "        for j in range(d):\n",
        "            if(i == 0):\n",
        "                col_name.append('w1,'+str(j))\n",
        "            w1 = np.random.binomial(n=1,p=build_w1(u1, u2))\n",
        "            scm_numpy[i][j] = w1\n",
        "            w1_s.append(w1)\n",
        "        w2 = np.random.binomial(n=1,p=build_w2(w1_s, d))\n",
        "        x = np.random.binomial(n=1,p=build_x(u1, w2))\n",
        "        y = build_y(u2, x)\n",
        "        scm_numpy[i][d] = w2\n",
        "        scm_numpy[i][d+1] = x\n",
        "        scm_numpy[i][d+2] = y\n",
        "        if(i == 0):\n",
        "            col_name.extend(['w2','x','y'])\n",
        "    # df = pd.DataFrame()\n",
        "    return scm_numpy ,col_name\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGRVkOVTbG9W"
      },
      "source": [
        "# DO on Surrogate(Ground Truth)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggsn7tGMa9Rd"
      },
      "source": [
        "def causal_do(scm,new_x):\n",
        "    print('bbbb',scm)\n",
        "    scm_copy = scm.copy()\n",
        "    scm_cols = np.size(scm_copy,axis=1)\n",
        "    scm_rows = np.size(scm_copy,axis=0)\n",
        "    print('scm_cols->',scm_cols)\n",
        "    scm_copy[:,scm_cols-2] = new_x\n",
        "    for i in range(scm_rows):\n",
        "        u2 = np.random.normal(loc=0,scale=1)\n",
        "        x = scm_copy[i,scm_cols-2]\n",
        "        scm_copy[i][scm_cols-1] = build_y(u2, x)\n",
        "\n",
        "    print('modified')\n",
        "    print(scm_copy)\n",
        "    return scm_copy\n",
        "\n",
        "\n",
        "def estimate_conditional_expectation1(df):\n",
        "    scm_cols = np.size(df,axis=1)\n",
        "    scm_rows = np.size(df,axis=0)\n",
        "    a = df[:,scm_cols-2] == 0\n",
        "    b = df[:,scm_cols-2] == 1\n",
        "    y = df[:,scm_cols-1]\n",
        "\n",
        "    p = np.mean(y[b])\n",
        "    q = np.mean(y[a])\n",
        "    delta = p-q\n",
        "    return p,q,delta\n",
        "def ab_test1(scm, x, y, n):\n",
        "    n_a = int(n / 2)\n",
        "    n_b = n - n_a\n",
        "    set_variable = np.array([0]*n_a + [1]*n_b)\n",
        "    scm = causal_do(scm,set_variable)\n",
        "    return estimate_conditional_expectation1(scm)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYftYCyLViIH"
      },
      "source": [
        "# Weighted\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKBgtu3eVpCs"
      },
      "source": [
        "def build_zs_t(d):\n",
        "    zs_unique = np.empty((d,2**d),int)\n",
        "    for i in range(d):\n",
        "        d_bar = 2**d/(2**(i+1))\n",
        "        d1 = 0\n",
        "        pp=1\n",
        "        for j in range(2**d):\n",
        "            if(pp == 1):\n",
        "                zs_unique[i][j] = 0\n",
        "                d1+=1\n",
        "                if(d1==d_bar):\n",
        "                    pp=-1\n",
        "                    d1=0\n",
        "            elif(pp == -1):\n",
        "                zs_unique[i][j] = 1\n",
        "                d1+=1\n",
        "                if(d1 == d_bar):\n",
        "                    pp=1\n",
        "                    d1=0\n",
        "\n",
        "    zs_t = zs_unique.transpose()\n",
        "    return zs_t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SB8i9MEAV1q9"
      },
      "source": [
        "def p_w_2(fd):\n",
        "    df_copy = fd['w2'].copy()\n",
        "    size = fd.shape[0]\n",
        "    pw2 = np.zeros(2)\n",
        "    prob_w2 = np.zeros(size)\n",
        "    for i in [0,1]:\n",
        "        nm1 = df_copy == i\n",
        "        pw2[i] = nm1.mean()\n",
        "    for i in range(size):\n",
        "        if df_copy[i] == 0:\n",
        "            prob_w2[i] = pw2[0]\n",
        "        else :\n",
        "            prob_w2[i] = pw2[1]\n",
        "    return prob_w2\n",
        "def p_w1_w2(df):\n",
        "    df_copy = df.copy()\n",
        "    d=df_copy.shape[1]-3\n",
        "    df_size = df.shape[0]\n",
        "    proba_z_x = np.ones(df.shape[0])\n",
        "    df_w1 = df_copy.iloc[:,0:d]\n",
        "    x_pr = df['w2']\n",
        "    X=df_w1.values.reshape(-1, d)\n",
        "    y=df['w2']\n",
        "    clf = LogisticRegression(random_state=0).fit(X, y)\n",
        "    predict = clf.predict_proba(X)\n",
        "    for j in range(df_size):\n",
        "      if ( x_pr[j] == 0):\n",
        "        proba_z_x[j] *= predict[j,0]\n",
        "      else :\n",
        "        proba_z_x[j] *= predict[j,1]\n",
        "    return proba_z_x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvvkrAWZRHld"
      },
      "source": [
        "def weightGenerator(df):\n",
        "  data1 = p_w_2(df)\n",
        "  data2 = p_w1_w2(df)\n",
        "  data_prob = np.divide(data1,data2)\n",
        "  return data_prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09TKD-SuRHd4"
      },
      "source": [
        "def cwoBeta(X,y,weight,beta):\n",
        "  Beta1 = LinearRegression().fit(X,y,sample_weight=weight)\n",
        "  test_x=np.array([[0,0], [1,1]]).reshape((-1, 2))\n",
        "  y_pred=Beta1.predict(test_x)\n",
        "\n",
        "  return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwNe1RoKV9KO"
      },
      "source": [
        "def weightedATE(df):\n",
        "  data=df.copy()\n",
        "  d=data.shape[1]-3\n",
        "  X=data.iloc[:,d:d+2].values.reshape(-1, 2)\n",
        "  Y=data['y'].values.reshape(-1, 1)\n",
        "  data['weight'] = weightGenerator(df)\n",
        "  ate=cwoBeta(X,Y,data['weight'],1)\n",
        "  return ate[1][0],ate[0][0],ate[1][0]-ate[0][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3UVF4tNY1LL"
      },
      "source": [
        "# Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4Hnaw38xVZG"
      },
      "source": [
        "hpDict={'conv_0_units': 40,\n",
        " 'dropout': 0.2,\n",
        " 'dropout_0_': 0.25,\n",
        " 'input_units': 60,\n",
        " 'learning_rate': 0.00925824604021082,\n",
        " 'n_layers': 1}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8htzojsOTot"
      },
      "source": [
        "def nnBeta(X,y,testX,weight,hp):\n",
        "  Beta2 = tf.keras.models.Sequential()\n",
        "  Beta2.add(tf.keras.layers.Dense(hp['input_units'],activation='relu',input_shape=(2,)))\n",
        "  Beta2.add(tf.keras.layers.Dropout(hp['dropout']))\n",
        "  for i in range(hp['n_layers']):\n",
        "    Beta2.add(tf.keras.layers.Dense(hp[f'conv_{i}_units'], activation='relu'))\n",
        "    Beta2.add(tf.keras.layers.Dropout(hp[f'dropout_{i}_']))\n",
        "\n",
        "  Beta2.add(tf.keras.layers.Dense(1, activation='linear'))\n",
        "  Beta2.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=hp['learning_rate']), loss='mae')\n",
        "\n",
        "  early_stop=tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=40, verbose=1, mode='auto', restore_best_weights=True)\n",
        "  history=Beta2.fit(X,y,epochs=1000,\n",
        "                    validation_split=0.2,\n",
        "                    shuffle = True,\n",
        "                    callbacks=[early_stop],\n",
        "                    verbose=0\n",
        "                    ,sample_weight=weight\n",
        "                    )\n",
        "\n",
        "\n",
        "  y_pred = Beta2.predict(testX)\n",
        "  return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9DHgZpoI8Le"
      },
      "source": [
        "def NeuralATE(df,hp):\n",
        "  data=df.copy()\n",
        "  d=data.shape[1]-3\n",
        "  X=data.iloc[:,d:d+2].values.reshape(-1, 2)\n",
        "  Y=data['y'].values.reshape(-1, 1)\n",
        "  sampleWeight = weightGenerator(df)\n",
        "  test=np.array([[1,0], [0,1]])\n",
        "  test_x=pd.DataFrame(test,columns=['a','b'])\n",
        "  XX=test_x.values.reshape(-1, 2)\n",
        "  ate = nnBeta(X,Y,XX,sampleWeight,hp)\n",
        "\n",
        "\n",
        "  return ate[1][0],ate[0][0],ate[1][0]-ate[0][0]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "De2Vj58qdF5e"
      },
      "source": [
        "# Hyper Parameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvvpE3TvdaDx"
      },
      "source": [
        "!pip install keras-tuner==1.0.0\n",
        "import kerastuner"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFNFieZeeR3K"
      },
      "source": [
        "from kerastuner.tuners import RandomSearch\n",
        "import time\n",
        "LOG_DIR = f\"{int(time.time())}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27ADZkzRdFR6"
      },
      "source": [
        "def tempNN(hp):\n",
        "  Beta2 = tf.keras.models.Sequential()\n",
        "  Beta2.add(tf.keras.layers.Dense(hp.Int('input_units',\n",
        "                                         min_value=10,\n",
        "                                         max_value=100,\n",
        "                                         step=10), activation='relu',input_shape=(2,)))\n",
        "  Beta2.add(tf.keras.layers.Dropout(hp.Float('dropout',\n",
        "                                             min_value=0.0,\n",
        "                                             max_value=0.4,\n",
        "                                             default=0.1,\n",
        "                                             step=0.05,)))\n",
        "  for i in range(hp.Int('n_layers', 0, 4)):\n",
        "    Beta2.add(tf.keras.layers.Dense(hp.Int(f'conv_{i}_units',\n",
        "                                           min_value=10,\n",
        "                                           max_value=100,\n",
        "                                           step=10), activation='relu'))\n",
        "    Beta2.add(tf.keras.layers.Dropout(hp.Float(f'dropout_{i}_',\n",
        "                                             min_value=0.0,\n",
        "                                             max_value=0.4,\n",
        "                                             default=0.1,\n",
        "                                             step=0.05,)))\n",
        "\n",
        "  Beta2.add(tf.keras.layers.Dense(1, activation='linear'))\n",
        "  Beta2.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=hp.Float('learning_rate',\n",
        "                                                                            min_value=1e-5,\n",
        "                                                                            max_value=1e-2,\n",
        "                                                                            sampling='LOG',\n",
        "                                                                            default=1e-3)), loss='mse')\n",
        "\n",
        "  return Beta2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9gCCsm3eYd5"
      },
      "source": [
        "tuner = RandomSearch(\n",
        "    tempNN,\n",
        "    objective='val_loss',\n",
        "    max_trials=5,  # how many model variations to test?\n",
        "    executions_per_trial=4,  # how many trials per variation? (same model could perform differently)\n",
        "    directory=LOG_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3zdY3P9XDhv"
      },
      "source": [
        "data=fd.sample(n=500).reset_index(drop=True)\n",
        "X=data.iloc[:,D:D+2].values.reshape(-1, 2)\n",
        "Y=data['y'].values.reshape(-1, 1)\n",
        "sampleWeight = weightGenerator(data)\n",
        "early_stop=tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=20, verbose=1, mode='auto', restore_best_weights=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIl4WAn6i384"
      },
      "source": [
        "tuner.search(X,Y,epochs=500,\n",
        "             validation_split=0.2,\n",
        "             shuffle = True,\n",
        "             callbacks=[early_stop],\n",
        "             verbose=2\n",
        "             ,sample_weight=sampleWeight\n",
        "         )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLAzDR9oJcig"
      },
      "source": [
        "tuner.get_best_hyperparameters()[0].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiQVVtuP6OF7"
      },
      "source": [
        "tuner.get_best_hyperparameters()[0].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utg6tC4t6N9I"
      },
      "source": [
        "tuner.get_best_models()[0].summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKKdONmGWbhK"
      },
      "source": [
        "def doDo(d):\n",
        "  N=10000000\n",
        "  scm_np ,col_name= build_causal_model(N, d)\n",
        "  mu1,mu0,muATE=ab_test1(scm_np,'x','y',N)\n",
        "  df = pd.DataFrame(scm_np,columns=col_name)\n",
        "  return df,mu1,mu0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXiiezmKXPIy"
      },
      "source": [
        "D=7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6zfwu2TXMcx"
      },
      "source": [
        "fd,mu1,mu0=doDo(D)\n",
        "print(\"Estimated ATE: {:.3f}\".format(mu1-mu0))\n",
        "print(fd)\n",
        "print(\"mu(1): {:.3f}\".format(mu1))\n",
        "print(\"mu(0): {:.3f}\".format(mu0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqaVofFwySKb"
      },
      "source": [
        "# Saving and loading pandas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JY7ChUBkC-I"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDmPMn1mwZBQ"
      },
      "source": [
        "fd.to_pickle(\"drive/MyDrive/Datasets/sg_df_7.pkl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWYymiikw9Zv"
      },
      "source": [
        "string1 = str(mu1) + \" \" + str(mu0)\n",
        "print(string1)\n",
        "f = open(\"drive/MyDrive/Datasets/sgmu_7.txt\",\"w\")\n",
        "f.write(string1)\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q01OAgiHZlWb"
      },
      "source": [
        "# Result Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "at7uuRuMmts0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grZKHoenx-Rt"
      },
      "source": [
        "fd = pd.read_pickle(\"/content/drive/MyDrive/Datasets/sg_df_7.pkl\")\n",
        "print(fd)\n",
        "f = open('/content/drive/MyDrive/Datasets/sgmu_7.txt','r')\n",
        "mm = f.read()\n",
        "mu1,mu0 = mm.split()\n",
        "mu1=float(mu1)\n",
        "mu0=float(mu0)\n",
        "print(mu1,mu0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1UirzssDHW_"
      },
      "source": [
        "def MAAEgenerator(fd,mu1,mu0):\n",
        "\n",
        "  muN1,muN0,_=NeuralATE(fd,hpDict)\n",
        "  muC1,muC0,_=weightedATE(fd)\n",
        "  maaeN=(abs(muN1-mu1)+abs(muN0-muN0))/2\n",
        "  maaeC=(abs(muC1-mu1)+abs(muC0-mu0))/2\n",
        "  return maaeN,maaeC\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9y0YZP5Z1KN"
      },
      "source": [
        "N_SAMPLE=500"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqQt5CkarYeD"
      },
      "source": [
        "for N_SAMPLE in range(4000,10001,500):\n",
        "  muN=np.zeros(100)\n",
        "  muC=np.zeros(100)\n",
        "  for k in range(100):\n",
        "    # print(\"K= \",k)\n",
        "    db=fd.sample(n=N_SAMPLE).reset_index(drop=True)\n",
        "    muN[k],muC[k]=MAAEgenerator(db,mu1,mu0)\n",
        "    # print(k,muN[k],muC[k])\n",
        "  print(N_SAMPLE)\n",
        "  MAAE_N=np.median(muN)\n",
        "  print(\"MAAE NN\",MAAE_N)\n",
        "  MAAE_C=np.median(muC)\n",
        "  print(\"MAAE CWO\",MAAE_C)\n",
        "  # print(\"MAAE NN: {:.5f}\".format(MAAE_N))\n",
        "  # print(\"MAAE CWO: {:.5f}\".format(MAAE_C))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}